{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2b5970e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, query_engine, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.memory import Memory\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from core.settings import CHROMA_PATH, llm, embed_model\n",
    "import chromadb,os\n",
    "from llama_index.core.storage.chat_store import SimpleChatStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a8cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"C:/Users/khalil/Desktop/fastapi_llamaindex/vector_stores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69c8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9ce2f787",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadTimeout",
     "evalue": "timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    125\u001b[39m exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[262]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:435\u001b[39m, in \u001b[36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    427\u001b[39m     CBEventType.LLM,\n\u001b[32m    428\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m     },\n\u001b[32m    433\u001b[39m )\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    437\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    438\u001b[39m         CBEventType.LLM,\n\u001b[32m    439\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    440\u001b[39m         event_id=event_id,\n\u001b[32m    441\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:557\u001b[39m, in \u001b[36mOllama.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;129m@llm_completion_callback\u001b[39m()\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcomplete\u001b[39m(\n\u001b[32m    555\u001b[39m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m    556\u001b[39m ) -> CompletionResponse:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchat_to_completion_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\llms\\generic_utils.py:184\u001b[39m, in \u001b[36mchat_to_completion_decorator.<locals>.wrapper\u001b[39m\u001b[34m(prompt, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> CompletionResponse:\n\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# normalize input\u001b[39;00m\n\u001b[32m    183\u001b[39m     messages = prompt_to_messages(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     chat_response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# normalize output\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:175\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    167\u001b[39m     CBEventType.LLM,\n\u001b[32m    168\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m     },\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    177\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    178\u001b[39m         CBEventType.LLM,\n\u001b[32m    179\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    180\u001b[39m         event_id=event_id,\n\u001b[32m    181\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:343\u001b[39m, in \u001b[36mOllama.chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m think = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mthink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.thinking\n\u001b[32m    341\u001b[39m \u001b[38;5;28mformat\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m response = \u001b[38;5;28mdict\u001b[39m(response)\n\u001b[32m    356\u001b[39m tool_calls = response[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\ollama\\_client.py:351\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    307\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    308\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    316\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    317\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    318\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    320\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\ollama\\_client.py:129\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    128\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m     r.raise_for_status()\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: timed out"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"hello\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc929c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"hello\")\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25663606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and ollama embedding function and will be used by chroma when creating collections.\n",
    "embed_fn =OllamaEmbeddingFunction(\n",
    "    url=\"http://192.168.2.62:11434\",\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a38b0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Ds-Terminal/Desktop/rag-llamaindex-chroma/vector_stores')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHROMA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8bf132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import settings\n",
    "settings.llm = llm\n",
    "settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46bb425a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_identifier': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\vector_stores',\n",
       " 'tenant': 'default_tenant',\n",
       " 'database': 'default_database',\n",
       " '_server': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_admin_client': <chromadb.api.client.AdminClient at 0x2a52407c690>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "vector_client.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5279d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=string), Collection(name=stringg)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddc8d1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('43417482-b4b8-4abc-aca9-c882f25be1cd'), name='stringg', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=768, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.get_collection(\"stringg\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c654843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load documents from a folder\n",
    "user_dir = \"./data/stringg\"\n",
    "docs = SimpleDirectoryReader(user_dir).load_data()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "user_name = \"stringg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d5f3bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='150835fd-0206-43f9-bbee-2fe316503054', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\stringg\\\\text.txt', 'file_name': 'text.txt', 'file_type': 'text/plain', 'file_size': 230, 'creation_date': '2025-10-08', 'last_modified_date': '2025-10-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1) llm:\\r\\nmodel=\"llama3.1\",\\r\\nbase_url=\"http://192.168.2.62:11434\"\\r\\n \\r\\n2) embeding model: BAAI/bge-base-en\\r\\n \\r\\n3) context handling: during chat it mantains the running history of  dialogues. \\r\\n \\r\\n4) we will provide you the document ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf8036a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('43417482-b4b8-4abc-aca9-c882f25be1cd'), name='stringg', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=768, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = vector_client.get_collection(user_name)\n",
    "collection.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3981b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore.from_collection(collection=collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e9afe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from C:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\vector_stores\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from C:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\vector_stores\\index_store.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'docstore': <llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore at 0x2a5246bd450>,\n",
       " 'index_store': <llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore at 0x2a5246bd810>,\n",
       " 'vector_stores': {'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})},\n",
       " 'graph_store': <llama_index.core.graph_stores.simple.SimpleGraphStore at 0x2a5246bd950>,\n",
       " 'property_graph_store': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 6️⃣ Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=CHROMA_PATH)\n",
    "storage_context.__dict__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38c7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#         vector_store=vector_store\n",
    "#     )\n",
    "# print(\"Index loaded from existing vector store!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef4a9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 00:06:00,763 - INFO - Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "index = load_index_from_storage(storage_context=storage_context)\n",
    "chat_engine = index.as_chat_engine(llm=llm, chat_mode=\"condense_plus_context\",system_prompt=(\"you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48345d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 23:39:55,437 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 23:39:55,439 - INFO - Condensed question: What other model names are mentioned in addition to \"llama3.1\"?\n",
      "2025-10-09 23:39:55,479 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 23:39:57,369 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"what model names are in the context?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70a5d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two model names mentioned in the context: \"llama3.1\" and \"BAAI/bge-base-en\".\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2fc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######integrating memroy to the chat_engine ##############3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1945ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new vector collection for the same user to store chat vectores\n",
    "collection_name = user_name + \"_chats\"\n",
    "chat_collection = vector_client.create_collection(collection_name, embedding_function=embed_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1b314da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('40be0135-131b-4ebf-83fc-a5c619748022'), name='stringg_chats', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=None, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.get_collection(collection_name).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8cee12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector store instance using the chat_collection\n",
    "chat_vector_store = ChromaVectorStore.from_collection(chat_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bae8ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "###defining static/ long term memroy\n",
    "from llama_index.core.memory import StaticMemoryBlock,FactExtractionMemoryBlock,VectorMemoryBlock\n",
    "memory_blocks=[\n",
    "    StaticMemoryBlock(\n",
    "    name=\"core-info\",\n",
    "    static_content=\"My name is khalil, I line in Islamabad, I work with llamaindex.\",\n",
    "    priority=0\n",
    "),\n",
    "\n",
    "FactExtractionMemoryBlock(\n",
    "    name=\"extracted-info\",\n",
    "    llm=llm,\n",
    "    max_facts=30,\n",
    "    priority=1\n",
    "),\n",
    "\n",
    "VectorMemoryBlock(\n",
    "    name = \"vector_memory\",\n",
    "    vector_store=chat_vector_store,\n",
    "    priority=2,\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=3,\n",
    "    retrieval_context_window=5,\n",
    "    \n",
    "\n",
    "),\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dir = \"chats\"\n",
    "chat_id=\"abc\"\n",
    "os.makedirs(chat_dir, exist_ok=True)\n",
    "\n",
    "# Create user folder\n",
    "user_chat_dir = os.path.join(chat_dir, \"notebook_chat1\")\n",
    "os.makedirs(user_chat_dir, exist_ok=True)\n",
    "\n",
    "# File path for chat\n",
    "user_chat_path = os.path.join(user_chat_dir, f\"{chat_id}.json\")\n",
    "\n",
    "# Create persistent SimpleChatStore\n",
    "chat_store = SimpleChatStore.from_persist_path(persist_path=user_chat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "20714bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chats': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='chat content...')])]}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a19d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.add_message(key=\"chats\",message = ChatMessage(role=\"system\", content=\"sytem some content..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c6634f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.persist(persist_path=user_chat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "06dcb323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chats': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='user message  chat content222...')]),\n",
       "  ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')])]}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "66765299",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_manual = 'sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "6573f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory.from_defaults(\n",
    "    session_id=user_name,\n",
    "    token_limit=100,  # Normally you would set this to be closer to the LLM context window (i.e. 75,000, etc.)\n",
    "    token_flush_size=20,\n",
    "    chat_history_token_ratio=0.7,\n",
    "    memory_blocks= memory_blocks,\n",
    "    async_database_uri= path_manual\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2b22f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 100,\n",
       " 'token_flush_size': 20,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [StaticMemoryBlock(name='core-info', description=None, priority=0, accept_short_term_memory=True, static_content=[TextBlock(block_type='text', text='My name is khalil, I line in Islamabad, I work with llamaindex.')]),\n",
       "  FactExtractionMemoryBlock(name='extracted-info', description=None, priority=1, accept_short_term_memory=True, llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002A51E3D0B80>, completion_to_prompt=<function default_completion_to_prompt at 0x000002A51EA4ACA0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None), facts=[], max_facts=30, fact_extraction_prompt_template=RichPromptTemplate(metadata={}, template_vars=['existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact extraction system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the conversation segment provided prior to this message\\n2. Extract specific, concrete facts the user has disclosed or important information discovered\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the extracted facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>'), fact_condense_prompt_template=RichPromptTemplate(metadata={}, template_vars=['max_facts', 'existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact condensing system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the current list of existing facts\\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the condensed facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>')),\n",
       "  VectorMemoryBlock(name='vector_memory', description=None, priority=2, accept_short_term_memory=True, vector_store=ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), embed_model=OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None), similarity_top_k=3, retrieval_context_window=5, format_template=RichPromptTemplate(metadata={}, template_vars=['text'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='{{ text }}'), node_postprocessors=[], query_kwargs={})],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'video_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3', db_schema=None),\n",
       " 'session_id': 'stringg'}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dda85012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='<memory>\\n<core-info>\\nMy name is khalil, I line in Islamabad, I work with llamaindex.\\n</core-info>\\n</memory>')])]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9f41f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine_with_memory = index.as_chat_engine(llm=llm,memory = memory ,chat_mode=\"condense_plus_context\",system_prompt=(\"you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "22d189e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 08:08:07,542 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:103\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:136\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:106\u001b[39m, in \u001b[36mAsyncHTTP11Connection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_response_headers(**kwargs)\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:177\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_async\\http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:32\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     26\u001b[39m exc_map = {\n\u001b[32m     27\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ReadTimeout,\n\u001b[32m     28\u001b[39m     anyio.BrokenResourceError: ReadError,\n\u001b[32m     29\u001b[39m     anyio.ClosedResourceError: ReadError,\n\u001b[32m     30\u001b[39m     anyio.EndOfStream: ReadError,\n\u001b[32m     31\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mReadTimeout\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[261]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m chat_engine_with_memory.achat(\u001b[33m\"\u001b[39m\u001b[33mwhat model names are given in the document?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:57\u001b[39m, in \u001b[36mtrace_method.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m callback_manager = cast(CallbackManager, callback_manager)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m callback_manager.as_trace(trace_id):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\chat_engine\\condense_plus_context.py:381\u001b[39m, in \u001b[36mCondensePlusContextChatEngine.achat\u001b[39m\u001b[34m(self, message, chat_history)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;129m@trace_method\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34machat\u001b[39m(\n\u001b[32m    379\u001b[39m     \u001b[38;5;28mself\u001b[39m, message: \u001b[38;5;28mstr\u001b[39m, chat_history: Optional[List[ChatMessage]] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    380\u001b[39m ) -> AgentChatResponse:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     synthesizer, context_source, context_nodes = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._arun_c3(\n\u001b[32m    382\u001b[39m         message, chat_history\n\u001b[32m    383\u001b[39m     )\n\u001b[32m    385\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m synthesizer.asynthesize(message, context_nodes)\n\u001b[32m    387\u001b[39m     user_message = ChatMessage(content=message, role=MessageRole.USER)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\chat_engine\\condense_plus_context.py:299\u001b[39m, in \u001b[36mCondensePlusContextChatEngine._arun_c3\u001b[39m\u001b[34m(self, message, chat_history, streaming)\u001b[39m\n\u001b[32m    296\u001b[39m chat_history = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._memory.aget(\u001b[38;5;28minput\u001b[39m=message)\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# Condense conversation history and latest message to a standalone question\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m condensed_question = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acondense_question(chat_history, message)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    300\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCondensed question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcondensed_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\chat_engine\\condense_plus_context.py:203\u001b[39m, in \u001b[36mCondensePlusContextChatEngine._acondense_question\u001b[39m\u001b[34m(self, chat_history, latest_message)\u001b[39m\n\u001b[32m    197\u001b[39m logger.debug(chat_history_str)\n\u001b[32m    199\u001b[39m llm_input = \u001b[38;5;28mself\u001b[39m._condense_prompt_template.format(\n\u001b[32m    200\u001b[39m     chat_history=chat_history_str, question=latest_message\n\u001b[32m    201\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llm.acomplete(llm_input))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:337\u001b[39m, in \u001b[36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_async_llm_predict\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    328\u001b[39m     CBEventType.LLM,\n\u001b[32m    329\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    333\u001b[39m     },\n\u001b[32m    334\u001b[39m )\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     f_return_val = \u001b[38;5;28;01mawait\u001b[39;00m f(_self, *args, **kwargs)\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    339\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    340\u001b[39m         CBEventType.LLM,\n\u001b[32m    341\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    342\u001b[39m         event_id=event_id,\n\u001b[32m    343\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:563\u001b[39m, in \u001b[36mOllama.acomplete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;129m@llm_completion_callback\u001b[39m()\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macomplete\u001b[39m(\n\u001b[32m    561\u001b[39m     \u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, formatted: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m    562\u001b[39m ) -> CompletionResponse:\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m achat_to_completion_decorator(\u001b[38;5;28mself\u001b[39m.achat)(prompt, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\llms\\generic_utils.py:232\u001b[39m, in \u001b[36machat_to_completion_decorator.<locals>.wrapper\u001b[39m\u001b[34m(prompt, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> CompletionResponse:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# normalize input\u001b[39;00m\n\u001b[32m    231\u001b[39m     messages = prompt_to_messages(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     chat_response = \u001b[38;5;28;01mawait\u001b[39;00m func(messages, **kwargs)\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# normalize output\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:76\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[39m\u001b[34m(_self, messages, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m     68\u001b[39m     CBEventType.LLM,\n\u001b[32m     69\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m     },\n\u001b[32m     74\u001b[39m )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     f_return_val = \u001b[38;5;28;01mawait\u001b[39;00m f(_self, messages, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     78\u001b[39m     callback_manager.on_event_end(\n\u001b[32m     79\u001b[39m         CBEventType.LLM,\n\u001b[32m     80\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m     81\u001b[39m         event_id=event_id,\n\u001b[32m     82\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:525\u001b[39m, in \u001b[36mOllama.achat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    522\u001b[39m think = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mthink\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.thinking\n\u001b[32m    523\u001b[39m \u001b[38;5;28mformat\u001b[39m = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.json_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.chat(\n\u001b[32m    526\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    527\u001b[39m     messages=ollama_messages,\n\u001b[32m    528\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    529\u001b[39m     \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    530\u001b[39m     tools=tools,\n\u001b[32m    531\u001b[39m     think=think,\n\u001b[32m    532\u001b[39m     options=\u001b[38;5;28mself\u001b[39m._model_kwargs,\n\u001b[32m    533\u001b[39m     keep_alive=\u001b[38;5;28mself\u001b[39m.keep_alive,\n\u001b[32m    534\u001b[39m )\n\u001b[32m    536\u001b[39m response = \u001b[38;5;28mdict\u001b[39m(response)\n\u001b[32m    538\u001b[39m tool_calls = response[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtool_calls\u001b[39m\u001b[33m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\ollama\\_client.py:953\u001b[39m, in \u001b[36mAsyncClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, think, format, options, keep_alive)\u001b[39m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    908\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    909\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    917\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    918\u001b[39m ) -> Union[ChatResponse, AsyncIterator[ChatResponse]]:\n\u001b[32m    919\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    920\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    921\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    950\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns an asynchronous `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m    954\u001b[39m     ChatResponse,\n\u001b[32m    955\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    956\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m/api/chat\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    957\u001b[39m     json=ChatRequest(\n\u001b[32m    958\u001b[39m       model=model,\n\u001b[32m    959\u001b[39m       messages=\u001b[38;5;28mlist\u001b[39m(_copy_messages(messages)),\n\u001b[32m    960\u001b[39m       tools=\u001b[38;5;28mlist\u001b[39m(_copy_tools(tools)),\n\u001b[32m    961\u001b[39m       stream=stream,\n\u001b[32m    962\u001b[39m       think=think,\n\u001b[32m    963\u001b[39m       \u001b[38;5;28mformat\u001b[39m=\u001b[38;5;28mformat\u001b[39m,\n\u001b[32m    964\u001b[39m       options=options,\n\u001b[32m    965\u001b[39m       keep_alive=keep_alive,\n\u001b[32m    966\u001b[39m     ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[32m    967\u001b[39m     stream=stream,\n\u001b[32m    968\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\ollama\\_client.py:751\u001b[39m, in \u001b[36mAsyncClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    747\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    749\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**(\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_raw(*args, **kwargs)).json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\ollama\\_client.py:691\u001b[39m, in \u001b[36mAsyncClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    690\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m     r = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.request(*args, **kwargs)\n\u001b[32m    692\u001b[39m     r.raise_for_status()\n\u001b[32m    693\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:1540\u001b[39m, in \u001b[36mAsyncClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1525\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m   1527\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1528\u001b[39m     method=method,\n\u001b[32m   1529\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1538\u001b[39m     extensions=extensions,\n\u001b[32m   1539\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1540\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(request, auth=auth, follow_redirects=follow_redirects)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mReadTimeout\u001b[39m: "
     ]
    }
   ],
   "source": [
    "result = await chat_engine_with_memory.achat(\"what model names are given in the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "eb73bbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided documents, two model names are mentioned:\\n\\n1. LLaMA: specifically, \"llama3.1\"\\n2. Embedding model: BAAI/bge-base-en'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c499540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'There are two model names mentioned in the context: \"llama3.1\" and \"BAAI/bge-base-en\".',\n",
       " 'sources': [ToolOutput(blocks=[TextBlock(block_type='text', text='[NodeWithScore(node=TextNode(id_=\\'2a2d6fb7-1e95-428e-abd2-a3ca05bf7da6\\', embedding=None, metadata={\\'file_path\\': \\'C:\\\\\\\\Users\\\\\\\\Ds-Terminal\\\\\\\\Desktop\\\\\\\\rag-llamaindex-chroma\\\\\\\\data\\\\\\\\stringg\\\\\\\\text.txt\\', \\'file_name\\': \\'text.txt\\', \\'file_type\\': \\'text/plain\\', \\'file_size\\': 230, \\'creation_date\\': \\'2025-10-08\\', \\'last_modified_date\\': \\'2025-10-08\\'}, excluded_embed_metadata_keys=[\\'file_name\\', \\'file_type\\', \\'file_size\\', \\'creation_date\\', \\'last_modified_date\\', \\'last_accessed_date\\'], excluded_llm_metadata_keys=[\\'file_name\\', \\'file_type\\', \\'file_size\\', \\'creation_date\\', \\'last_modified_date\\', \\'last_accessed_date\\'], relationships={<NodeRelationship.SOURCE: \\'1\\'>: RelatedNodeInfo(node_id=\\'4a42901e-e12b-475b-8f08-5e60556094bf\\', node_type=\\'4\\', metadata={\\'file_path\\': \\'C:\\\\\\\\Users\\\\\\\\Ds-Terminal\\\\\\\\Desktop\\\\\\\\rag-llamaindex-chroma\\\\\\\\data\\\\\\\\stringg\\\\\\\\text.txt\\', \\'file_name\\': \\'text.txt\\', \\'file_type\\': \\'text/plain\\', \\'file_size\\': 230, \\'creation_date\\': \\'2025-10-08\\', \\'last_modified_date\\': \\'2025-10-08\\'}, hash=\\'c352f36093033b346e385ab192da77af3e90fccb038762e942f68860e92fdc88\\')}, metadata_template=\\'{key}: {value}\\', metadata_separator=\\'\\\\n\\', text=\\'1) llm:\\\\r\\\\nmodel=\"llama3.1\",\\\\r\\\\nbase_url=\"http://192.168.2.62:11434\"\\\\r\\\\n \\\\r\\\\n2) embeding model: BAAI/bge-base-en\\\\r\\\\n \\\\r\\\\n3) context handling: during chat it mantains the running history of  dialogues. \\\\r\\\\n \\\\r\\\\n4) we will provide you the document\\', mimetype=\\'text/plain\\', start_char_idx=0, end_char_idx=229, metadata_seperator=\\'\\\\n\\', text_template=\\'{metadata_str}\\\\n\\\\n{content}\\'), score=0.6653037788665801)]')], tool_name='retriever', raw_input={'message': 'What other model names are mentioned in addition to \"llama3.1\"?'}, raw_output=[NodeWithScore(node=TextNode(id_='2a2d6fb7-1e95-428e-abd2-a3ca05bf7da6', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\stringg\\\\text.txt', 'file_name': 'text.txt', 'file_type': 'text/plain', 'file_size': 230, 'creation_date': '2025-10-08', 'last_modified_date': '2025-10-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a42901e-e12b-475b-8f08-5e60556094bf', node_type='4', metadata={'file_path': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\stringg\\\\text.txt', 'file_name': 'text.txt', 'file_type': 'text/plain', 'file_size': 230, 'creation_date': '2025-10-08', 'last_modified_date': '2025-10-08'}, hash='c352f36093033b346e385ab192da77af3e90fccb038762e942f68860e92fdc88')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='1) llm:\\r\\nmodel=\"llama3.1\",\\r\\nbase_url=\"http://192.168.2.62:11434\"\\r\\n \\r\\n2) embeding model: BAAI/bge-base-en\\r\\n \\r\\n3) context handling: during chat it mantains the running history of  dialogues. \\r\\n \\r\\n4) we will provide you the document', mimetype='text/plain', start_char_idx=0, end_char_idx=229, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6653037788665801)], is_error=False)],\n",
       " 'source_nodes': [NodeWithScore(node=TextNode(id_='2a2d6fb7-1e95-428e-abd2-a3ca05bf7da6', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\stringg\\\\text.txt', 'file_name': 'text.txt', 'file_type': 'text/plain', 'file_size': 230, 'creation_date': '2025-10-08', 'last_modified_date': '2025-10-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4a42901e-e12b-475b-8f08-5e60556094bf', node_type='4', metadata={'file_path': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\stringg\\\\text.txt', 'file_name': 'text.txt', 'file_type': 'text/plain', 'file_size': 230, 'creation_date': '2025-10-08', 'last_modified_date': '2025-10-08'}, hash='c352f36093033b346e385ab192da77af3e90fccb038762e942f68860e92fdc88')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='1) llm:\\r\\nmodel=\"llama3.1\",\\r\\nbase_url=\"http://192.168.2.62:11434\"\\r\\n \\r\\n2) embeding model: BAAI/bge-base-en\\r\\n \\r\\n3) context handling: during chat it mantains the running history of  dialogues. \\r\\n \\r\\n4) we will provide you the document', mimetype='text/plain', start_char_idx=0, end_char_idx=229, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6653037788665801)],\n",
       " 'is_dummy_stream': False,\n",
       " 'metadata': None}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "35210f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 100,\n",
       " 'token_flush_size': 20,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [StaticMemoryBlock(name='core-info', description=None, priority=0, accept_short_term_memory=True, static_content=[TextBlock(block_type='text', text='My name is khalil, I line in Islamabad, I work with llamaindex.')]),\n",
       "  FactExtractionMemoryBlock(name='extracted-info', description=None, priority=1, accept_short_term_memory=True, llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002A51E3D0B80>, completion_to_prompt=<function default_completion_to_prompt at 0x000002A51EA4ACA0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None), facts=['No new facts were disclosed or important information was discovered.', 'My model is \"llama3.1\".', 'I\\'m connected to the base URL \"http://192.168.2.62:11434\".', 'My embedding model is BAAI/bge-base-en.', 'You have three children.', 'You previously lived in Karachi.', 'The capital of France is Paris.'], max_facts=30, fact_extraction_prompt_template=RichPromptTemplate(metadata={}, template_vars=['existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact extraction system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the conversation segment provided prior to this message\\n2. Extract specific, concrete facts the user has disclosed or important information discovered\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the extracted facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>'), fact_condense_prompt_template=RichPromptTemplate(metadata={}, template_vars=['max_facts', 'existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact condensing system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the current list of existing facts\\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the condensed facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>')),\n",
       "  VectorMemoryBlock(name='vector_memory', description=None, priority=2, accept_short_term_memory=True, vector_store=ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), embed_model=OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None), similarity_top_k=3, retrieval_context_window=5, format_template=RichPromptTemplate(metadata={}, template_vars=['text'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='{{ text }}'), node_postprocessors=[], query_kwargs={'filters': MetadataFilters(filters=[MetadataFilter(key='session_id', value='stringg', operator=<FilterOperator.EQ: '=='>)], condition=<FilterCondition.AND: 'and'>)})],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'video_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///:memory:', db_schema=None),\n",
       " 'session_id': 'stringg'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27499b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
