{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2b5970e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, query_engine, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.memory import Memory\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from core.settings import CHROMA_PATH, llm, embed_model\n",
    "import chromadb,os\n",
    "from llama_index.core.storage.chat_store import SimpleChatStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83a8cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"C:/Users/khalil/Desktop/fastapi_llamaindex/vector_stores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a69c8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f7f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "9ce2f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:35:45,404 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"hello\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "dc929c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 23:06:11,702 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"hello\")\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25663606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and ollama embedding function and will be used by chroma when creating collections.\n",
    "embed_fn =OllamaEmbeddingFunction(\n",
    "    url=\"http://192.168.2.62:11434\",\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a38b0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Ds-Terminal/Desktop/rag-llamaindex-chroma/vector_stores')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHROMA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8bf132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import settings\n",
    "settings.llm = llm\n",
    "settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46bb425a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_identifier': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\vector_stores',\n",
       " 'tenant': 'default_tenant',\n",
       " 'database': 'default_database',\n",
       " '_server': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_admin_client': <chromadb.api.client.AdminClient at 0x2a52407c690>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "vector_client.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5279d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=string), Collection(name=stringg)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddc8d1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('43417482-b4b8-4abc-aca9-c882f25be1cd'), name='stringg', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=768, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.get_collection(\"stringg\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c654843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load documents from a folder\n",
    "user_dir = \"./data/stringg\"\n",
    "docs = SimpleDirectoryReader(user_dir).load_data()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "user_name = \"stringg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d5f3bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='150835fd-0206-43f9-bbee-2fe316503054', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\stringg\\\\text.txt', 'file_name': 'text.txt', 'file_type': 'text/plain', 'file_size': 230, 'creation_date': '2025-10-08', 'last_modified_date': '2025-10-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='1) llm:\\r\\nmodel=\"llama3.1\",\\r\\nbase_url=\"http://192.168.2.62:11434\"\\r\\n \\r\\n2) embeding model: BAAI/bge-base-en\\r\\n \\r\\n3) context handling: during chat it mantains the running history of  dialogues. \\r\\n \\r\\n4) we will provide you the document ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf8036a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('43417482-b4b8-4abc-aca9-c882f25be1cd'), name='stringg', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=768, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = vector_client.get_collection(user_name)\n",
    "collection.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3981b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ChromaVectorStore.from_collection(collection=collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e9afe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from C:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\vector_stores\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from C:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\vector_stores\\index_store.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'docstore': <llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore at 0x2a5246bd450>,\n",
       " 'index_store': <llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore at 0x2a5246bd810>,\n",
       " 'vector_stores': {'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})},\n",
       " 'graph_store': <llama_index.core.graph_stores.simple.SimpleGraphStore at 0x2a5246bd950>,\n",
       " 'property_graph_store': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 6️⃣ Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=CHROMA_PATH)\n",
    "storage_context.__dict__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a38c7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_vector_store(\n",
    "#         vector_store=vector_store\n",
    "#     )\n",
    "# print(\"Index loaded from existing vector store!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef4a9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 00:06:00,763 - INFO - Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "index = load_index_from_storage(storage_context=storage_context)\n",
    "chat_engine = index.as_chat_engine(llm=llm, chat_mode=\"condense_plus_context\",system_prompt=(\"you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48345d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 23:39:55,437 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 23:39:55,439 - INFO - Condensed question: What other model names are mentioned in addition to \"llama3.1\"?\n",
      "2025-10-09 23:39:55,479 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 23:39:57,369 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"what model names are in the context?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70a5d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two model names mentioned in the context: \"llama3.1\" and \"BAAI/bge-base-en\".\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2fc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######integrating memroy to the chat_engine ##############3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1945ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new vector collection for the same user to store chat vectores\n",
    "collection_name = user_name + \"_chats\"\n",
    "chat_collection = vector_client.create_collection(collection_name, embedding_function=embed_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1b314da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('40be0135-131b-4ebf-83fc-a5c619748022'), name='stringg_chats', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=None, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.get_collection(collection_name).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8cee12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector store instance using the chat_collection\n",
    "chat_vector_store = ChromaVectorStore.from_collection(chat_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bae8ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "###defining static/ long term memroy\n",
    "from llama_index.core.memory import StaticMemoryBlock,FactExtractionMemoryBlock,VectorMemoryBlock\n",
    "memory_blocks=[\n",
    "    StaticMemoryBlock(\n",
    "    name=\"core-info\",\n",
    "    static_content=\"My name is khalil, I line in Islamabad, I work with llamaindex.\",\n",
    "    priority=0\n",
    "),\n",
    "\n",
    "FactExtractionMemoryBlock(\n",
    "    name=\"extracted-info\",\n",
    "    llm=llm,\n",
    "    max_facts=30,\n",
    "    priority=1\n",
    "),\n",
    "\n",
    "VectorMemoryBlock(\n",
    "    name = \"vector_memory\",\n",
    "    vector_store=chat_vector_store,\n",
    "    priority=2,\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=3,\n",
    "    retrieval_context_window=5,\n",
    "    \n",
    "\n",
    "),\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dir = \"chats\"\n",
    "chat_id=\"abc\"\n",
    "os.makedirs(chat_dir, exist_ok=True)\n",
    "\n",
    "# Create user folder\n",
    "user_chat_dir = os.path.join(chat_dir, \"notebook_chat1\")\n",
    "os.makedirs(user_chat_dir, exist_ok=True)\n",
    "\n",
    "# File path for chat\n",
    "user_chat_path = os.path.join(user_chat_dir, f\"{chat_id}.json\")\n",
    "\n",
    "# Create persistent SimpleChatStore\n",
    "chat_store = SimpleChatStore.from_persist_path(persist_path=user_chat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "20714bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chats': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='user message  chat content222...')]),\n",
       "  ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')])]}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a19d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.add_message(key=\"chats\",message = ChatMessage(role=\"system\", content=\"sytem some content..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c6634f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.persist(persist_path=user_chat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "06dcb323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chats': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='user message  chat content222...')]),\n",
       "  ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')])]}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66765299",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_manual = 'sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3'\n",
    "path = \"sqlite+aiosqlite:///C:/Users/Ds-Terminal/Desktop/rag-llamaindex-chroma/chats/string/abc.sqlite3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory.from_defaults(\n",
    "    session_id=user_name,\n",
    "    token_limit=100,  # Normally you would set this to be closer to the LLM context window (i.e. 75,000, etc.)\n",
    "    token_flush_size=20,\n",
    "    chat_history_token_ratio=0.7,\n",
    "    memory_blocks= memory_blocks,\n",
    "    async_database_uri= path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2b22f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 100,\n",
       " 'token_flush_size': 20,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [StaticMemoryBlock(name='core-info', description=None, priority=0, accept_short_term_memory=True, static_content=[TextBlock(block_type='text', text='My name is khalil, I line in Islamabad, I work with llamaindex.')]),\n",
       "  FactExtractionMemoryBlock(name='extracted-info', description=None, priority=1, accept_short_term_memory=True, llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002A51E3D0B80>, completion_to_prompt=<function default_completion_to_prompt at 0x000002A51EA4ACA0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None), facts=[], max_facts=30, fact_extraction_prompt_template=RichPromptTemplate(metadata={}, template_vars=['existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact extraction system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the conversation segment provided prior to this message\\n2. Extract specific, concrete facts the user has disclosed or important information discovered\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the extracted facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>'), fact_condense_prompt_template=RichPromptTemplate(metadata={}, template_vars=['max_facts', 'existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact condensing system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the current list of existing facts\\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the condensed facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>')),\n",
       "  VectorMemoryBlock(name='vector_memory', description=None, priority=2, accept_short_term_memory=True, vector_store=ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), embed_model=OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None), similarity_top_k=3, retrieval_context_window=5, format_template=RichPromptTemplate(metadata={}, template_vars=['text'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='{{ text }}'), node_postprocessors=[], query_kwargs={'filters': MetadataFilters(filters=[MetadataFilter(key='session_id', value='stringg', operator=<FilterOperator.EQ: '=='>)], condition=<FilterCondition.AND: 'and'>)})],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'video_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3', db_schema=None),\n",
       " 'session_id': 'stringg'}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "dda85012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='<memory>\\n<core-info>\\nMy name is khalil, I line in Islamabad, I work with llamaindex.\\n</core-info>\\n</memory>')])]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "9f41f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine_with_memory = index.as_chat_engine(llm=llm,memory = memory ,chat_mode=\"condense_plus_context\",system_prompt=(\"you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "22d189e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-12 21:59:57,242 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-12 22:00:11,245 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-12 22:00:11,249 - INFO - Condensed question: What is the capital of Pakistan?\n",
      "2025-10-12 22:00:13,491 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-12 22:00:30,780 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-12 22:00:33,176 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-12 22:00:50,566 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = await chat_engine_with_memory.achat(\"can YOu tell me the capital of Pakistan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "eb73bbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to my running history of dialogues (which includes your initial information that you live in Islamabad), I know that Islamabad is the capital of Pakistan! So, the answer is Islamabad.'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6c499540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\",\n",
       " 'additional_kwargs': {'tool_calls': [], 'thinking': None},\n",
       " 'raw': {'model': 'llama3:latest',\n",
       "  'created_at': '2025-10-13T09:25:35.003652639Z',\n",
       "  'done': True,\n",
       "  'done_reason': 'stop',\n",
       "  'total_duration': 16188640513,\n",
       "  'load_duration': 7120691371,\n",
       "  'prompt_eval_count': 11,\n",
       "  'prompt_eval_duration': 331030749,\n",
       "  'eval_count': 26,\n",
       "  'eval_duration': 8734816347,\n",
       "  'message': Message(role='assistant', content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", thinking=None, images=None, tool_name=None, tool_calls=None),\n",
       "  'usage': {'prompt_tokens': 11, 'completion_tokens': 26, 'total_tokens': 37}},\n",
       " 'logprobs': None,\n",
       " 'delta': None}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "35210f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 100,\n",
       " 'token_flush_size': 20,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [StaticMemoryBlock(name='core-info', description=None, priority=0, accept_short_term_memory=True, static_content=[TextBlock(block_type='text', text='My name is khalil, I line in Islamabad, I work with llamaindex.')]),\n",
       "  FactExtractionMemoryBlock(name='extracted-info', description=None, priority=1, accept_short_term_memory=True, llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002A51E3D0B80>, completion_to_prompt=<function default_completion_to_prompt at 0x000002A51EA4ACA0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None), facts=[], max_facts=30, fact_extraction_prompt_template=RichPromptTemplate(metadata={}, template_vars=['existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact extraction system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the conversation segment provided prior to this message\\n2. Extract specific, concrete facts the user has disclosed or important information discovered\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the extracted facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>'), fact_condense_prompt_template=RichPromptTemplate(metadata={}, template_vars=['max_facts', 'existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact condensing system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the current list of existing facts\\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the condensed facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>')),\n",
       "  VectorMemoryBlock(name='vector_memory', description=None, priority=2, accept_short_term_memory=True, vector_store=ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), embed_model=OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None), similarity_top_k=3, retrieval_context_window=5, format_template=RichPromptTemplate(metadata={}, template_vars=['text'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='{{ text }}'), node_postprocessors=[], query_kwargs={'filters': MetadataFilters(filters=[MetadataFilter(key='session_id', value='stringg', operator=<FilterOperator.EQ: '=='>)], condition=<FilterCondition.AND: 'and'>)})],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'video_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3', db_schema=None),\n",
       " 'session_id': 'stringg'}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27499b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
