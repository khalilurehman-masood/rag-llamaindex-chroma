{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2b5970e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, query_engine, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.memory import Memory\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "from core.settings import CHROMA_PATH, llm, embed_model\n",
    "import chromadb,os\n",
    "from llama_index.core.storage.chat_store import SimpleChatStore\n",
    "from llama_index.core.retrievers import VectorIndexRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a8cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"C:/Users/khalil/Desktop/fastapi_llamaindex/vector_stores\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce2f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"hello\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc929c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"hello\")\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25663606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define and ollama embedding function and will be used by chroma when creating collections.\n",
    "embed_fn =OllamaEmbeddingFunction(\n",
    "    url=\"http://192.168.2.62:11434\",\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a38b0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Ds-Terminal/Desktop/rag-llamaindex-chroma/vector_stores')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHROMA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8bf132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import settings\n",
    "settings.llm = llm\n",
    "settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb425a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 04:18:23,984 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_identifier': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\vector_stores',\n",
       " 'tenant': 'default_tenant',\n",
       " 'database': 'default_database',\n",
       " '_server': <chromadb.api.rust.RustBindingsAPI at 0x1cda3327b60>,\n",
       " '_admin_client': <chromadb.api.client.AdminClient at 0x1cda3327e00>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "vector_client.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5279d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=icd_abcd_chat),\n",
       " Collection(name=umar),\n",
       " Collection(name=hive_abcd_chat),\n",
       " Collection(name=icd),\n",
       " Collection(name=sikandarf),\n",
       " Collection(name=yasirrrr),\n",
       " Collection(name=sikandar_abcd_chat),\n",
       " Collection(name=hive),\n",
       " Collection(name=yasirr),\n",
       " Collection(name=yasirrr),\n",
       " Collection(name=yasirrrr_abcd_chat),\n",
       " Collection(name=umar_abcd_chat),\n",
       " Collection(name=sikandarf_abcd_chat)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc8d1ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Collection [stringg] does not exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvector_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstringg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.\u001b[34m__dict__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\chromadb\\api\\client.py:201\u001b[39m, in \u001b[36mClient.get_collection\u001b[39m\u001b[34m(self, name, embedding_function, data_loader)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    199\u001b[39m     data_loader: Optional[DataLoader[Loadable]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    200\u001b[39m ) -> Collection:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     persisted_ef_config = model.configuration_json.get(\u001b[33m\"\u001b[39m\u001b[33membedding_function\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m     validate_embedding_function_conflict_on_get(\n\u001b[32m    209\u001b[39m         embedding_function, persisted_ef_config\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\chromadb\\api\\rust.py:250\u001b[39m, in \u001b[36mRustBindingsAPI.get_collection\u001b[39m\u001b[34m(self, name, tenant, database)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_collection\u001b[39m(\n\u001b[32m    245\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    248\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    249\u001b[39m ) -> CollectionModel:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     collection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionModel(\n\u001b[32m    252\u001b[39m         \u001b[38;5;28mid\u001b[39m=collection.id,\n\u001b[32m    253\u001b[39m         name=collection.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m         database=collection.database,\n\u001b[32m    259\u001b[39m     )\n",
      "\u001b[31mNotFoundError\u001b[39m: Collection [stringg] does not exists"
     ]
    }
   ],
   "source": [
    "vector_client.get_collection(\"stringg\").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c654843a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 35 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load documents from a folder\n",
    "user_dir = \"./data/hive\"\n",
    "docs = SimpleDirectoryReader(user_dir).load_data()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "user_name = \"icd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c992341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id_': 'f7844132-9efb-4192-be0e-8ad2b92cd7c7',\n",
       " 'embedding': None,\n",
       " 'metadata': {'page_label': '1',\n",
       "  'file_name': 'HiveWorx_HR_Policy_Manual.pdf',\n",
       "  'file_path': 'c:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\hive\\\\HiveWorx_HR_Policy_Manual.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 1048607,\n",
       "  'creation_date': '2025-10-14',\n",
       "  'last_modified_date': '2025-10-14'},\n",
       " 'excluded_embed_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'excluded_llm_metadata_keys': ['file_name',\n",
       "  'file_type',\n",
       "  'file_size',\n",
       "  'creation_date',\n",
       "  'last_modified_date',\n",
       "  'last_accessed_date'],\n",
       " 'relationships': {},\n",
       " 'metadata_template': '{key}: {value}',\n",
       " 'metadata_separator': '\\n',\n",
       " 'text_resource': MediaResource(embeddings=None, data=None, text='                                                                                            HR POLICY MANUAL     \\nPage 1 of 35 \\n \\n  \\n  \\n \\n \\nHUMAN RESOURCE POLICY MANUAL \\n                                      Islamabad Office \\n                                 Effective July, 2019 \\n', path=None, url=None, mimetype=None),\n",
       " 'image_resource': None,\n",
       " 'audio_resource': None,\n",
       " 'video_resource': None,\n",
       " 'text_template': '{metadata_str}\\n\\n{content}'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8b669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf8036a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x1cda3327b60>,\n",
       " '_model': Collection(id=UUID('38904e78-c830-4934-8c61-24c16a35bc8b'), name='icd', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=768, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x1cd9f272b50>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = vector_client.get_collection(user_name)\n",
    "collection.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3981b7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stores_text': True,\n",
       " 'is_embedding_query': True,\n",
       " 'flat_metadata': True,\n",
       " 'collection_name': None,\n",
       " 'host': None,\n",
       " 'port': None,\n",
       " 'ssl': False,\n",
       " 'headers': None,\n",
       " 'persist_dir': None,\n",
       " 'collection_kwargs': {}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = ChromaVectorStore.from_collection(collection=collection)\n",
    "vector_store.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6a3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseComponent.to_dict of ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7803eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 05:26:27,770 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = vector_store._collection.query(query_texts=[\"misconduct\"], include=[\"embeddings\",\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "456b6aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5bfcccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[\"documents\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e9afe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from C:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\vector_stores\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from C:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\vector_stores\\index_store.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'docstore': <llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore at 0x1cda3397df0>,\n",
       " 'index_store': <llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore at 0x1cda33945a0>,\n",
       " 'vector_stores': {'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={})},\n",
       " 'graph_store': <llama_index.core.graph_stores.simple.SimpleGraphStore at 0x1cda3397570>,\n",
       " 'property_graph_store': None}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 6️⃣ Create storage context\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=CHROMA_PATH)\n",
    "storage_context.__dict__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a38c7841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded from existing vector store!\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "print(\"Index loaded from existing vector store!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "beafeb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_use_async': False,\n",
       " '_store_nodes_override': False,\n",
       " '_embed_model': OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001CDA0B3B750>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None),\n",
       " '_insert_batch_size': 2048,\n",
       " '_storage_context': StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x000001CDA336AA50>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x000001CDA338B850>, vector_stores={'default': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x000001CDA6DC5E50>, property_graph_store=None),\n",
       " '_docstore': <llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore at 0x1cda336aa50>,\n",
       " '_show_progress': False,\n",
       " '_vector_store': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}),\n",
       " '_graph_store': <llama_index.core.graph_stores.simple.SimpleGraphStore at 0x1cda6dc5e50>,\n",
       " '_callback_manager': <llama_index.core.callbacks.base.CallbackManager at 0x1cda0b3b750>,\n",
       " '_object_map': {},\n",
       " '_index_struct': IndexDict(index_id='060b7ef0-6fc6-4ed2-9186-c83cc85e1ffa', summary=None, nodes_dict={}, doc_id_dict={}, embeddings_dict={}),\n",
       " '_transformations': [SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001CDA0B3B750>, id_func=<function default_id_func at 0x000001CD999BE160>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?|[,.;。？！]')]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72140cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 21:41:25,012 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m retriever = VectorIndexRetriever(index=index, similarity_top_k=\u001b[32m5\u001b[39m, vector_store_kwargs={\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]})\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nodes = \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmisconduct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\base\\base_retriever.py:210\u001b[39m, in \u001b[36mBaseRetriever.retrieve\u001b[39m\u001b[34m(self, str_or_query_bundle)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.as_trace(\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    207\u001b[39m         CBEventType.RETRIEVE,\n\u001b[32m    208\u001b[39m         payload={EventPayload.QUERY_STR: query_bundle.query_str},\n\u001b[32m    209\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m         nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m         nodes = \u001b[38;5;28mself\u001b[39m._handle_recursive_retrieval(query_bundle, nodes)\n\u001b[32m    212\u001b[39m         retrieve_event.on_end(\n\u001b[32m    213\u001b[39m             payload={EventPayload.NODES: nodes},\n\u001b[32m    214\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:104\u001b[39m, in \u001b[36mVectorIndexRetriever._retrieve\u001b[39m\u001b[34m(self, query_bundle)\u001b[39m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle.embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle.embedding_strs) > \u001b[32m0\u001b[39m:\n\u001b[32m     99\u001b[39m         query_bundle.embedding = (\n\u001b[32m    100\u001b[39m             \u001b[38;5;28mself\u001b[39m._embed_model.get_agg_embedding_from_queries(\n\u001b[32m    101\u001b[39m                 query_bundle.embedding_strs\n\u001b[32m    102\u001b[39m             )\n\u001b[32m    103\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_nodes_with_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_bundle\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\retrievers\\retriever.py:220\u001b[39m, in \u001b[36mVectorIndexRetriever._get_nodes_with_embeddings\u001b[39m\u001b[34m(self, query_bundle_with_embeddings)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_nodes_with_embeddings\u001b[39m(\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m, query_bundle_with_embeddings: QueryBundle\n\u001b[32m    218\u001b[39m ) -> List[NodeWithScore]:\n\u001b[32m    219\u001b[39m     query = \u001b[38;5;28mself\u001b[39m._build_vector_store_query(query_bundle_with_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     nodes_to_fetch = \u001b[38;5;28mself\u001b[39m._determine_nodes_to_fetch(query_result)\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nodes_to_fetch:\n\u001b[32m    224\u001b[39m         \u001b[38;5;66;03m# Fetch any missing nodes from the docstore and insert them into the query result\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:396\u001b[39m, in \u001b[36mChromaVectorStore.query\u001b[39m\u001b[34m(self, query, **kwargs)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query.mode == VectorStoreQueryMode.MMR:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mmr_search(query, where, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ds-Terminal\\Desktop\\rag-llamaindex-chroma\\.venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:420\u001b[39m, in \u001b[36mChromaVectorStore._query\u001b[39m\u001b[34m(self, query_embeddings, n_results, where, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    414\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._collection.query(\n\u001b[32m    415\u001b[39m         query_embeddings=query_embeddings,\n\u001b[32m    416\u001b[39m         n_results=n_results,\n\u001b[32m    417\u001b[39m         **kwargs,\n\u001b[32m    418\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m> Top \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m nodes:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    421\u001b[39m nodes = []\n\u001b[32m    422\u001b[39m similarities = []\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=5, vector_store_kwargs={\"include\":[\"embeddings\"],})\n",
    "nodes = retriever.retrieve(\"misconduct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3d51bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': <llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x1cda337b4d0>,\n",
       " '_vector_store': ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}),\n",
       " '_embed_model': OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001CDA0B3B750>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None),\n",
       " '_docstore': <llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore at 0x1cda32a5e50>,\n",
       " '_similarity_top_k': 2,\n",
       " '_vector_store_query_mode': <VectorStoreQueryMode.DEFAULT: 'default'>,\n",
       " '_alpha': None,\n",
       " '_node_ids': [],\n",
       " '_doc_ids': None,\n",
       " '_filters': None,\n",
       " '_sparse_top_k': None,\n",
       " '_hybrid_top_k': None,\n",
       " '_kwargs': {},\n",
       " 'callback_manager': <llama_index.core.callbacks.base.CallbackManager at 0x1cda0b3b750>,\n",
       " 'object_map': {},\n",
       " '_verbose': False}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6b1bb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='f6a99a67-9dc8-4b4e-80e9-e728d00f2c7d', embedding=None, metadata={'page_label': '17', 'file_name': 'HiveWorx_HR_Policy_Manual.pdf', 'file_path': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\icd\\\\HiveWorx_HR_Policy_Manual.pdf', 'file_type': 'application/pdf', 'file_size': 1048607, 'creation_date': '2025-10-15', 'last_modified_date': '2025-10-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ab60e883-f7dc-49a4-bc82-3b8807dd1000', node_type='4', metadata={'page_label': '17', 'file_name': 'HiveWorx_HR_Policy_Manual.pdf', 'file_path': 'C:\\\\Users\\\\Ds-Terminal\\\\Desktop\\\\rag-llamaindex-chroma\\\\data\\\\icd\\\\HiveWorx_HR_Policy_Manual.pdf', 'file_type': 'application/pdf', 'file_size': 1048607, 'creation_date': '2025-10-15', 'last_modified_date': '2025-10-15'}, hash='62e6c2c1dfa52e3f377e482ad3c88f93e06cbe4b1fc323b3bda5f93825d9e1ab'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='16bd134b-b6ac-4c6a-856c-b6bb59d9e8e3', node_type='1', metadata={}, hash='bdcbda12e4ee5e2b96cfc653d2d804c5112e78eddd101ae5df98a7ad7493d8e2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='HR POLICY MANUAL     \\nPage 17 of 35 \\n \\n \\n \\n10.1 What is Misconduct? \\nThe following acts and omissions shall be treated as misconduct: \\n1. Willful defiance or disobedience, whether alone or  in combination with others, to any \\nlawful and reasonable order of a superior; \\n2. Theft, fraud or dishonesty in connection with office business or property; \\n3. Willful damage to or loss of the office goods or property; \\n4. Using the position personally to obtain money or other valuables or to borrow funds; \\n5. Habitual absence without leave or absence without leave for more than two (02) days; \\n6. Habitual late attendance; \\n7. Breach of any law applicable to the office; \\n8. Riotous or disorderly behaviors during working hours at the office or any acts subversive \\nof discipline; \\n9. Habitual negligence or neglect of work; \\n10. Tampering of official secrets; \\n11. Strike, go-slow or inciting to do the same; \\n12.', mimetype='text/plain', start_char_idx=92, end_char_idx=1002, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7747312612340606)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef4a9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 05:35:25,915 - INFO - Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "index = load_index_from_storage(storage_context=storage_context)\n",
    "chat_engine = index.as_chat_engine(llm=llm, chat_mode=\"condense_plus_context\",system_prompt=(\"you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "77088293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_retriever': <llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever at 0x1cda44fb750>,\n",
       " '_llm': Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001CDA0B3B750>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001CD9BE136A0>, completion_to_prompt=<function default_completion_to_prompt at 0x000001CD9C4D1A80>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None),\n",
       " '_memory': ChatMemoryBuffer(chat_store=SimpleChatStore(store={'chat_history': []}), chat_store_key='chat_history', token_limit=7936, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all')),\n",
       " '_context_prompt_template': PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  {context_str}\\n\\n  Instruction: Based on the above documents, provide a detailed answer for the user question below.\\n  Answer \"don\\'t know\" if not present in the document.\\n  '),\n",
       " '_context_refine_prompt_template': PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_msg', 'existing_answer'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"\\n  The following is a friendly conversation between a user and an AI assistant.\\n  The assistant is talkative and provides lots of specific details from its context.\\n  If the assistant does not know the answer to a question, it truthfully says it\\n  does not know.\\n\\n  Here are the relevant documents for the context:\\n\\n  {context_msg}\\n\\n  Existing Answer:\\n  {existing_answer}\\n\\n  Instruction: Refine the existing answer using the provided context to assist the user.\\n  If the context isn't helpful, just repeat the existing answer and nothing more.\\n  \"),\n",
       " '_condense_prompt_template': PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['chat_history', 'question'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='\\n  Given the following conversation between a user and an AI assistant and a follow up question from user,\\n  rephrase the follow up question to be a standalone question.\\n\\n  Chat History:\\n  {chat_history}\\n  Follow Up Input: {question}\\n  Standalone question:'),\n",
       " '_system_prompt': 'you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.',\n",
       " '_skip_condense': False,\n",
       " '_node_postprocessors': [],\n",
       " 'callback_manager': <llama_index.core.callbacks.base.CallbackManager at 0x1cda0b3b750>,\n",
       " '_token_counter': <llama_index.core.utilities.token_counting.TokenCounter at 0x1cda44fae90>,\n",
       " '_verbose': False}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48345d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 23:39:55,437 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 23:39:55,439 - INFO - Condensed question: What other model names are mentioned in addition to \"llama3.1\"?\n",
      "2025-10-09 23:39:55,479 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-09 23:39:57,369 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"what model names are in the context?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70a5d3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are two model names mentioned in the context: \"llama3.1\" and \"BAAI/bge-base-en\".\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2fc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######integrating memroy to the chat_engine ##############3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "1945ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new vector collection for the same user to store chat vectores\n",
    "collection_name = user_name + \"_chats\"\n",
    "chat_collection = vector_client.create_collection(collection_name, embedding_function=embed_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1b314da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_client': <chromadb.api.rust.RustBindingsAPI at 0x2a5219f5550>,\n",
       " '_model': Collection(id=UUID('40be0135-131b-4ebf-83fc-a5c619748022'), name='stringg_chats', configuration_json={'hnsw': {'space': 'cosine', 'ef_construction': 100, 'ef_search': 100, 'max_neighbors': 16, 'resize_factor': 1.2, 'sync_threshold': 1000}, 'spann': None, 'embedding_function': {'type': 'known', 'name': 'ollama', 'config': {'model_name': 'nomic-embed-text:latest', 'timeout': 60, 'url': 'http://192.168.2.62:11434'}}}, metadata=None, dimension=None, tenant='default_tenant', database='default_database', version=0, log_position=0),\n",
       " '_embedding_function': <chromadb.utils.embedding_functions.DefaultEmbeddingFunction at 0x2a5213dd850>,\n",
       " '_data_loader': None}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_client.get_collection(collection_name).__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8cee12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a vector store instance using the chat_collection\n",
    "chat_vector_store = ChromaVectorStore.from_collection(chat_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bae8ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "###defining static/ long term memroy\n",
    "from llama_index.core.memory import StaticMemoryBlock,FactExtractionMemoryBlock,VectorMemoryBlock\n",
    "memory_blocks=[\n",
    "    StaticMemoryBlock(\n",
    "    name=\"core-info\",\n",
    "    static_content=\"My name is khalil, I line in Islamabad, I work with llamaindex.\",\n",
    "    priority=0\n",
    "),\n",
    "\n",
    "FactExtractionMemoryBlock(\n",
    "    name=\"extracted-info\",\n",
    "    llm=llm,\n",
    "    max_facts=30,\n",
    "    priority=1\n",
    "),\n",
    "\n",
    "VectorMemoryBlock(\n",
    "    name = \"vector_memory\",\n",
    "    vector_store=chat_vector_store,\n",
    "    priority=2,\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=3,\n",
    "    retrieval_context_window=5,\n",
    "    \n",
    "\n",
    "),\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_dir = \"chats\"\n",
    "chat_id=\"abc\"\n",
    "os.makedirs(chat_dir, exist_ok=True)\n",
    "\n",
    "# Create user folder\n",
    "user_chat_dir = os.path.join(chat_dir, \"notebook_chat1\")\n",
    "os.makedirs(user_chat_dir, exist_ok=True)\n",
    "\n",
    "# File path for chat\n",
    "user_chat_path = os.path.join(user_chat_dir, f\"{chat_id}.json\")\n",
    "\n",
    "# Create persistent SimpleChatStore\n",
    "chat_store = SimpleChatStore.from_persist_path(persist_path=user_chat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "20714bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chats': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='user message  chat content222...')]),\n",
       "  ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')])]}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a19d8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.add_message(key=\"chats\",message = ChatMessage(role=\"system\", content=\"sytem some content..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c6634f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_store.persist(persist_path=user_chat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "06dcb323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chats': [ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='user message  chat content222...')]),\n",
       "  ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')]),\n",
       "  ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='sytem some content..')])]}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_store.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "66765299",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_manual = 'sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3'\n",
    "path = \"sqlite+aiosqlite:///C:/Users/Ds-Terminal/Desktop/rag-llamaindex-chroma/chats/string/abc.sqlite3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "6573f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory.from_defaults(\n",
    "    session_id=user_name,\n",
    "    token_limit=100,  # Normally you would set this to be closer to the LLM context window (i.e. 75,000, etc.)\n",
    "    token_flush_size=20,\n",
    "    chat_history_token_ratio=0.7,\n",
    "    memory_blocks= memory_blocks,\n",
    "    async_database_uri= path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2b22f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 100,\n",
       " 'token_flush_size': 20,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [StaticMemoryBlock(name='core-info', description=None, priority=0, accept_short_term_memory=True, static_content=[TextBlock(block_type='text', text='My name is khalil, I line in Islamabad, I work with llamaindex.')]),\n",
       "  FactExtractionMemoryBlock(name='extracted-info', description=None, priority=1, accept_short_term_memory=True, llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002A51E3D0B80>, completion_to_prompt=<function default_completion_to_prompt at 0x000002A51EA4ACA0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None), facts=['Model names mentioned include \"llama3.1\" and BAAI/bge-base-en.'], max_facts=30, fact_extraction_prompt_template=RichPromptTemplate(metadata={}, template_vars=['existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact extraction system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the conversation segment provided prior to this message\\n2. Extract specific, concrete facts the user has disclosed or important information discovered\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the extracted facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>'), fact_condense_prompt_template=RichPromptTemplate(metadata={}, template_vars=['max_facts', 'existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact condensing system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the current list of existing facts\\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the condensed facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>')),\n",
       "  VectorMemoryBlock(name='vector_memory', description=None, priority=2, accept_short_term_memory=True, vector_store=ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), embed_model=OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None), similarity_top_k=3, retrieval_context_window=5, format_template=RichPromptTemplate(metadata={}, template_vars=['text'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='{{ text }}'), node_postprocessors=[], query_kwargs={'filters': MetadataFilters(filters=[MetadataFilter(key='session_id', value='stringg', operator=<FilterOperator.EQ: '=='>)], condition=<FilterCondition.AND: 'and'>)})],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'video_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///C:/Users/Ds-Terminal/Desktop/rag-llamaindex-chroma/chats/string/abc.sqlite3', db_schema=None),\n",
       " 'session_id': 'stringg'}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "dda85012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='<memory>\\n<core-info>\\nMy name is khalil, I line in Islamabad, I work with llamaindex.\\n</core-info>\\n<extracted-info>\\n<fact>Model names mentioned include \"llama3.1\" and BAAI/bge-base-en.</fact>\\n</extracted-info>\\n</memory>')])]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "9f41f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine_with_memory = index.as_chat_engine(llm=llm,memory = memory ,chat_mode=\"condense_plus_context\",system_prompt=(\"you are a helpful assistant that can answer general questions as well as quetions asked from the provided context. Please be concise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "22d189e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 00:10:28,280 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 00:10:32,517 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 00:10:32,520 - INFO - Condensed question: Here's the rephrased standalone question:\n",
      "\n",
      "What is the capital of Pakistan?\n",
      "2025-10-13 00:10:32,652 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-10-13 00:10:35,563 - INFO - HTTP Request: POST http://192.168.2.62:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "result = await chat_engine_with_memory.achat(\"can YOu tell me the capital of Pakistan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "eb73bbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A question that's not specifically related to the provided context! But don't worry, I'm here to help. According to my general knowledge, the capital of Pakistan is Islamabad. Would you like to know more about Islamabad or Pakistan in general?\""
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6c499540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\",\n",
       " 'additional_kwargs': {'tool_calls': [], 'thinking': None},\n",
       " 'raw': {'model': 'llama3:latest',\n",
       "  'created_at': '2025-10-13T09:25:35.003652639Z',\n",
       "  'done': True,\n",
       "  'done_reason': 'stop',\n",
       "  'total_duration': 16188640513,\n",
       "  'load_duration': 7120691371,\n",
       "  'prompt_eval_count': 11,\n",
       "  'prompt_eval_duration': 331030749,\n",
       "  'eval_count': 26,\n",
       "  'eval_duration': 8734816347,\n",
       "  'message': Message(role='assistant', content=\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\", thinking=None, images=None, tool_name=None, tool_calls=None),\n",
       "  'usage': {'prompt_tokens': 11, 'completion_tokens': 26, 'total_tokens': 37}},\n",
       " 'logprobs': None,\n",
       " 'delta': None}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "35210f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_limit': 100,\n",
       " 'token_flush_size': 20,\n",
       " 'chat_history_token_ratio': 0.7,\n",
       " 'memory_blocks': [StaticMemoryBlock(name='core-info', description=None, priority=0, accept_short_term_memory=True, static_content=[TextBlock(block_type='text', text='My name is khalil, I line in Islamabad, I work with llamaindex.')]),\n",
       "  FactExtractionMemoryBlock(name='extracted-info', description=None, priority=1, accept_short_term_memory=True, llm=Ollama(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002A51E3D0B80>, completion_to_prompt=<function default_completion_to_prompt at 0x000002A51EA4ACA0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, base_url='http://192.168.2.62:11434', model='llama3:latest', temperature=None, context_window=8192, request_timeout=30.0, prompt_key='prompt', json_mode=False, additional_kwargs={}, is_function_calling_model=True, keep_alive=None, thinking=None), facts=[], max_facts=30, fact_extraction_prompt_template=RichPromptTemplate(metadata={}, template_vars=['existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact extraction system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the conversation segment provided prior to this message\\n2. Extract specific, concrete facts the user has disclosed or important information discovered\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the extracted facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>'), fact_condense_prompt_template=RichPromptTemplate(metadata={}, template_vars=['max_facts', 'existing_facts'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='You are a precise fact condensing system designed to identify key information from conversations.\\n\\nINSTRUCTIONS:\\n1. Review the current list of existing facts\\n2. Condense the facts into a more concise list, less than {{ max_facts }} facts\\n3. Focus on factual information like preferences, personal details, requirements, constraints, or context\\n4. Format each fact as a separate <fact> XML tag\\n5. Do not include opinions, summaries, or interpretations - only extract explicit information\\n6. Do not duplicate facts that are already in the existing facts list\\n\\n<existing_facts>\\n{{ existing_facts }}\\n</existing_facts>\\n\\nReturn ONLY the condensed facts in this exact format:\\n<facts>\\n  <fact>Specific fact 1</fact>\\n  <fact>Specific fact 2</fact>\\n  <!-- More facts as needed -->\\n</facts>\\n\\nIf no new facts are present, return: <facts></facts>')),\n",
       "  VectorMemoryBlock(name='vector_memory', description=None, priority=2, accept_short_term_memory=True, vector_store=ChromaVectorStore(stores_text=True, is_embedding_query=True, flat_metadata=True, collection_name=None, host=None, port=None, ssl=False, headers=None, persist_dir=None, collection_kwargs={}), embed_model=OllamaEmbedding(model_name='nomic-embed-text:latest', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002A5219CDA70>, num_workers=None, embeddings_cache=None, base_url='http://192.168.2.62:11434', ollama_additional_kwargs={}, query_instruction=None, text_instruction=None), similarity_top_k=3, retrieval_context_window=5, format_template=RichPromptTemplate(metadata={}, template_vars=['text'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='{{ text }}'), node_postprocessors=[], query_kwargs={'filters': MetadataFilters(filters=[MetadataFilter(key='session_id', value='stringg', operator=<FilterOperator.EQ: '=='>)], condition=<FilterCondition.AND: 'and'>)})],\n",
       " 'memory_blocks_template': RichPromptTemplate(metadata={}, template_vars=['memory_blocks'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template_str='\\n<memory>\\n{% for (block_name, block_content) in memory_blocks %}\\n<{{ block_name }}>\\n  {% for block in block_content %}\\n    {% if block.block_type == \"text\" %}\\n{{ block.text }}\\n    {% elif block.block_type == \"image\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | image }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | image }}\\n      {% endif %}\\n    {% elif block.block_type == \"audio\" %}\\n      {% if block.url %}\\n        {{ (block.url | string) | audio }}\\n      {% elif block.path %}\\n        {{ (block.path | string) | audio }}\\n      {% endif %}\\n    {% endif %}\\n  {% endfor %}\\n</{{ block_name }}>\\n{% endfor %}\\n</memory>\\n'),\n",
       " 'insert_method': <InsertMethod.SYSTEM: 'system'>,\n",
       " 'image_token_size_estimate': 256,\n",
       " 'audio_token_size_estimate': 256,\n",
       " 'video_token_size_estimate': 256,\n",
       " 'tokenizer_fn': functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'),\n",
       " 'sql_store': SQLAlchemyChatStore(table_name='llama_index_memory', async_database_uri='sqlite+aiosqlite:///chats/notebook_chat/abc.sqlite3', db_schema=None),\n",
       " 'session_id': 'stringg'}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27499b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
